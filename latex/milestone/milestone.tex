	\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{Image Completion: Milestone}

\author{Debora Sujono, Yue Tang \& Grace Yoo \\ 
University of Massachusetts Amherst \\ 
College of Information \& Computer Sciences \\
{\tt\small \{dsujono, gyoo, ytang\}@cs.umass.edu} \\
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% BODY TEXT
\section{Introduction}
%Introduction: this section introduces your problem, and the overall plan for approaching your problem
Image completion, which is also called content-aware fill or inpainting, is a generative technique that can be used to fill missing areas of an image. The desired result is an image that a human would perceive as normal. These techniques are often implemented in photo-editing software to mend corrupted images.

\par We can conceptualize a photograph as a low-dimensional sample from a high-dimensional distribution, the original scene. The problem can be framed as learning these high-dimensional distributions such that we can generate a sample that approximates missing pixels. Contextual information from incomplete images, such as the values of pixels neighboring the missing content, can help inform the generative model. On a higher level, perceptual information, such as understanding what a normal image looks like, is necessary to generate images that humans will believe. %% cite blog?

\par We will build a generative model to create samples with which to fill in missing areas of images. A generative model typically takes some hidden parameters and then randomly generates values from a joint probability distribution that covers labeled observations. In the context of our problem, a generative model will learn the hidden parameters that determine normal image structures and simulate values for all pixels of an image.

\section{Problem statement}
%Problem statement: Describe your problem precisely specifying the dataset to be used, expected results and evaluation
Given an image with a square region of missing pixels, our goal is to fill in the missing pixels to create a cohesive image. We will use the CIFAR-10 dataset, which consists of $60,000$ images evenly distributed into 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck). Each $32$ x $32$ x $3$ pixel image is represented by the RGB color model. We will remove a square region of each image, and consider the original image to be the original distribution of $32$ x $32$ x $3 = 3,072$ dimensions. 

\par As our baseline, we will implement a convolutional neural net (CNN) with mean squared error (MSE) loss between incomplete and original images, to perform regression on the intensities of the missing pixels. From here, we will each investigate different generative modeling methods to improve the baseline score using the same evaluation.

\par After experimentation, we will optimize one or two models that we find most promising. We will use Root Mean Squared Error (RMSE) to compare models. The RMSE will give us an idea of how well our generated images compare to the originals; however, the problem is to generate any plausible images, so we will also randomly sample images for human judgment. We will ask judges to rate an image as "plausible" or "not plausible", and determine which models generate the most believable images. 


\section{Technical Approach}
%Technical Approach: Describe the methods you intend to apply to solve the given problem
Training a generative model can easily become intractable; we not only have to simulate high dimensional distributions, but also apply some kind of inference method. In the following sections, we will outline the probabilistic underpinnings of the different training methods that we will implement. Let $p(x)$ be the generated distribution for image $x$.

\subsection{Generative Adversarial Networks}
Generative Adversarial Networks (GANs) train two networks simultaneously; a discriminator $D(x)$ that says whether an image $x$ is an original or generated, and a generator that produces input $x$ for the discriminator. The generator network takes a parameter $Z$, a random sample from a distribution (such as Gaussian), and generates image $G(Z)$.

\par The discriminator outputs a scalar $D(x)$ for each image $x$; let's say that for an original image, $D(x)$ is low, while for generated images $D(x) = D(G(Z))$ is high. The discriminator's objective is simply to adjust parameters to score images accurately; in other words, to maximize $D(x)$.

The generator, on the other hand, wants to produce an image $G(Z)$ such that the discriminator thinks it's an original, so it wants to minimize $D(G(Z))$. In this sense, we can describe the generator and discriminator as two players in a minimax game \cite{GAN}



\subsection{Deep Convolutional Generative Adversarial Networks}
Deep Convolutional Generative Adversarial Networks (DCGAN) aim to scale GANs by using CNNs and adding some extra constraints to help stabilize training. Here are the guidelines outlined by 

\subsection{PixelCNN}
\subsection{PixelRNN}

\subsection{Expectations}
We expect that images from the GAN will be rather noisy

\subsection{Image completion with perceptual and contextual losses}

\section{Preliminary Results}
%Intermediate/Preliminary Results: State and evaluate your results upto the milestone
sdfasdfsd \cite{Alpher02}

\subsection{References}

{\small
\bibliographystyle{ieee}
\bibliography{milestone}
}

%\begin{thebibliography}{1}
%\bibitem{image_gen} Conditional Image Generation with PixelCNN Decoders \url{https://arxiv.org/pdf/1606.05328v2.pdf}
%\bibitem{pix_net} Pixel Recurrent Neural Networks \url{https://arxiv.org/pdf/1601.06759v3}
%\bibitem{GAN} Generative Adversarial Nets \url{http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf}
%\bibitem{convGAN} Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks \url{https://arxiv.org/pdf/1511.06434v2}
%\bibitem{inpaint} Semantic Image Inpainting with Perceptual and Contextual Losses \url{https://arxiv.org/pdf/1607.07539v2.pdf}
%\end{thebibliography}


\end{document}
