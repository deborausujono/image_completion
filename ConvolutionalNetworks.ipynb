{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Networks\n",
    "So far we have worked with deep fully-connected networks, using them to explore different optimization strategies and network architectures. Fully-connected networks are a good testbed for experimentation because they are very computationally efficient, but in practice all state-of-the-art results use convolutional networks instead.\n",
    "\n",
    "First you will implement several layer types that are used in convolutional networks. You will then use these layers to train a convolutional network on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from code.classifiers.cnn import *\n",
    "from code.data_utils import get_CIFAR10_data\n",
    "from code.gradient_check import eval_numerical_gradient_array, eval_numerical_gradient\n",
    "from code.layers import *\n",
    "from code.fast_layers import *\n",
    "from code.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[  40.35810204   38.94430612   37.03944898 ...,   52.08416327\n",
      "      49.74489796   50.94502041]\n",
      "   [  39.97585714   38.73944898   36.86344898 ...,   56.07028571\n",
      "      52.7807551    50.94636735]\n",
      "   [  41.33663265   39.34353061   38.58165306 ...,   52.87120408\n",
      "      51.48989796   50.48187755]\n",
      "   ..., \n",
      "   [ -13.53626531   -9.56961224   -6.90812245 ...,   -5.28104082\n",
      "      -4.71422449   -6.54308163]\n",
      "   [ -16.05942857  -13.44416327  -12.2555102  ...,   -9.53432653\n",
      "     -10.6054898   -13.01536735]\n",
      "   [ -25.73597959  -22.58926531  -21.85330612 ...,  -15.8035102\n",
      "     -16.56571429  -15.64218367]]\n",
      "\n",
      "  [[  51.01826531   51.65195918   50.85671429 ...,   57.82467347\n",
      "      54.4197551    56.63465306]\n",
      "   [  48.78316327   49.63691837   49.91895918 ...,   54.96422449   54.544\n",
      "      55.70804082]\n",
      "   [  50.41510204   50.57302041   51.01312245 ...,   55.11087755\n",
      "      54.52940816   56.50165306]\n",
      "   ..., \n",
      "   [   7.06446939   11.28759184   14.21993878 ...,   18.95071429\n",
      "      18.22269388   16.1052449 ]\n",
      "   [   4.63295918    6.44293878    7.84238776 ...,   13.61495918\n",
      "      12.32565306    9.68942857]\n",
      "   [  -5.95240816   -2.64944898   -0.75191837 ...,    7.26855102\n",
      "       6.37042857    7.13804082]]\n",
      "\n",
      "  [[  81.52608163   82.24597959   81.52363265 ...,   79.70193878\n",
      "      77.1307551    78.21483673]\n",
      "   [  79.72267347   79.63771429   79.00018367 ...,   79.21777551\n",
      "      77.61238776   78.64302041]\n",
      "   [  80.81738776   80.05812245   80.59912245 ...,   79.79559184\n",
      "      78.02171429   78.84563265]\n",
      "   ..., \n",
      "   [  34.86355102   39.22930612   42.274      ...,   44.08228571\n",
      "      44.21557143   41.86181633]\n",
      "   [  32.35404082   34.32144898   35.84595918 ...,   39.70189796\n",
      "      38.25883673   35.37018367]\n",
      "   [  20.54336735   24.02361224   25.05283673 ...,   33.137        32.08042857\n",
      "      32.60042857]]]\n",
      "\n",
      "\n",
      " [[[ 124.35810204  123.94430612  120.03944898 ...,  123.08416327\n",
      "     122.74489796  124.94502041]\n",
      "   [ 123.97585714  123.73944898  122.86344898 ...,  123.07028571\n",
      "     122.7807551   125.94636735]\n",
      "   [ 125.33663265  124.34353061  124.58165306 ...,  124.87120408\n",
      "     125.48989796  126.48187755]\n",
      "   ..., \n",
      "   [ 128.46373469  129.43038776  130.09187755 ...,  129.71895918\n",
      "     129.28577551  129.45691837]\n",
      "   [ 127.94057143  128.55583673  128.7444898  ...,  128.46567347\n",
      "     128.3945102   128.98463265]\n",
      "   [ 127.26402041  127.41073469  127.14669388 ...,  127.1964898\n",
      "     127.43428571  128.35781633]]\n",
      "\n",
      "  [[ 119.01826531  116.65195918  115.85671429 ...,  117.82467347\n",
      "     117.4197551   119.63465306]\n",
      "   [ 119.78316327  118.63691837  116.91895918 ...,  117.96422449  117.544\n",
      "     120.70804082]\n",
      "   [ 120.41510204  119.57302041  120.01312245 ...,  120.11087755\n",
      "     120.52940816  121.50165306]\n",
      "   ..., \n",
      "   [ 129.06446939  130.28759184  131.21993878 ...,  130.95071429\n",
      "     130.22269388  130.1052449 ]\n",
      "   [ 128.63295918  129.44293878  129.84238776 ...,  129.61495918\n",
      "     129.32565306  129.68942857]\n",
      "   [ 128.04759184  128.35055102  128.24808163 ...,  128.26855102\n",
      "     128.37042857  129.13804082]]\n",
      "\n",
      "  [[ 117.52608163  118.24597959  121.52363265 ...,  121.70193878\n",
      "     121.1307551   123.21483673]\n",
      "   [ 120.72267347  116.63771429  119.00018367 ...,  122.21777551\n",
      "     121.61238776  124.64302041]\n",
      "   [ 124.81738776  123.05812245  118.59912245 ...,  124.79559184\n",
      "     125.02171429  125.84563265]\n",
      "   ..., \n",
      "   [ 140.86355102  142.22930612  143.274      ...,  143.08228571\n",
      "     142.21557143  141.86181633]\n",
      "   [ 140.35404082  141.32144898  141.84595918 ...,  141.70189796\n",
      "     141.25883673  141.37018367]\n",
      "   [ 139.54336735  140.02361224  140.05283673 ...,  140.137       140.08042857\n",
      "     140.60042857]]]\n",
      "\n",
      "\n",
      " [[[  23.35810204   26.94430612   24.03944898 ...,  -10.91583673\n",
      "      -5.25510204    1.94502041]\n",
      "   [  21.97585714   21.73944898   20.86344898 ...,  -11.92971429\n",
      "      -2.2192449    -3.05363265]\n",
      "   [  16.33663265   10.34353061   10.58165306 ...,   21.87120408\n",
      "      17.48989796   14.48187755]\n",
      "   ..., \n",
      "   [ -36.53626531  -46.56961224  -30.90812245 ...,   -4.28104082\n",
      "     -35.71422449  -60.54308163]\n",
      "   [ -43.05942857  -59.44416327  -39.2555102  ...,   31.46567347\n",
      "      10.3945102   -30.01536735]\n",
      "   [ -74.73597959  -54.58926531  -21.85330612 ...,    4.1964898\n",
      "     -11.56571429  -15.64218367]]\n",
      "\n",
      "  [[  10.01826531   17.65195918   16.85671429 ...,  -16.17532653\n",
      "      -9.5802449    -3.36534694]\n",
      "   [  10.78316327   14.63691837   14.91895918 ...,  -19.03577551   -8.456\n",
      "      -9.29195918]\n",
      "   [   8.41510204    5.57302041    8.01312245 ...,   13.11087755\n",
      "       9.52940816    5.50165306]\n",
      "   ..., \n",
      "   [ -35.93553061  -45.71240816  -29.78006122 ...,   -3.04928571\n",
      "     -34.77730612  -59.8947551 ]\n",
      "   [ -42.36704082  -58.55706122  -38.15761224 ...,   32.61495918\n",
      "      11.32565306  -29.31057143]\n",
      "   [ -73.95240816  -53.64944898  -20.75191837 ...,    5.26855102\n",
      "     -10.62957143  -14.86195918]]\n",
      "\n",
      "  [[  13.52608163   20.24597959   18.52363265 ...,  -12.29806122\n",
      "      -6.8692449     0.21483673]\n",
      "   [  14.72267347   16.63771429   17.00018367 ...,  -13.78222449\n",
      "      -4.38761224   -5.35697959]\n",
      "   [  11.81738776    8.05812245    9.59912245 ...,   18.79559184\n",
      "      15.02171429   10.84563265]\n",
      "   ..., \n",
      "   [ -24.13644898  -33.77069388  -17.726      ...,    9.08228571\n",
      "     -22.78442857  -48.13818367]\n",
      "   [ -30.64595918  -46.67855102  -26.15404082 ...,   44.70189796\n",
      "      23.25883673  -17.62981633]\n",
      "   [ -62.45663265  -41.97638776   -9.94716327 ...,   17.137         1.08042857\n",
      "      -3.39957143]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[ -95.64189796  -90.05569388  -88.96055102 ...,  -31.91583673\n",
      "     -51.25510204  -41.05497959]\n",
      "   [ -73.02414286  -85.26055102  -80.13655102 ...,   26.07028571\n",
      "      11.7807551   -13.05363265]\n",
      "   [ -31.66336735  -64.65646939  -60.41834694 ...,   58.87120408\n",
      "      -9.51010204  -67.51812245]\n",
      "   ..., \n",
      "   [ -53.53626531  -71.56961224  -69.90812245 ..., -107.28104082\n",
      "    -103.71422449  -92.54308163]\n",
      "   [ -66.05942857  -70.44416327  -68.2555102  ..., -101.53432653\n",
      "    -108.6054898  -119.01536735]\n",
      "   [ -83.73597959  -80.58926531  -77.85330612 ...,  -99.8035102\n",
      "    -105.56571429 -114.64218367]]\n",
      "\n",
      "  [[  42.01826531   40.65195918   39.85671429 ...,   40.82467347\n",
      "      11.4197551    12.63465306]\n",
      "   [  46.78316327   49.63691837   47.91895918 ...,   46.96422449   42.544\n",
      "      14.70804082]\n",
      "   [  62.41510204   55.57302041   58.01312245 ...,   61.11087755\n",
      "       1.52940816  -54.49834694]\n",
      "   ..., \n",
      "   [ -46.93553061  -60.71240816  -54.78006122 ...,  -83.04928571\n",
      "     -87.77730612  -76.8947551 ]\n",
      "   [ -58.36704082  -54.55706122  -45.15761224 ...,  -76.38504082\n",
      "     -89.67434694 -102.31057143]\n",
      "   [ -70.95240816  -59.64944898  -48.75191837 ...,  -73.73144898\n",
      "     -82.62957143  -94.86195918]]\n",
      "\n",
      "  [[ 102.52608163  107.24597959  108.52363265 ...,   86.70193878\n",
      "      65.1307551    57.21483673]\n",
      "   [ 102.72267347  119.63771429  109.00018367 ...,   69.21777551\n",
      "      75.61238776   44.64302041]\n",
      "   [ 106.81738776  123.05812245  115.59912245 ...,   76.79559184\n",
      "      18.02171429  -39.15436735]\n",
      "   ..., \n",
      "   [ -37.13644898  -43.77069388  -30.726      ...,  -46.91771429\n",
      "     -60.78442857  -64.13818367]\n",
      "   [ -39.64595918  -26.67855102   -9.15404082 ...,  -40.29810204\n",
      "     -59.74116327  -81.62981633]\n",
      "   [ -42.45663265  -25.97638776   -8.94716327 ...,  -36.863       -47.91957143\n",
      "     -64.39957143]]]\n",
      "\n",
      "\n",
      " [[[  58.35810204   55.94430612   54.03944898 ...,   44.08416327\n",
      "      41.74489796   38.94502041]\n",
      "   [  63.97585714   61.73944898   59.86344898 ...,   43.07028571\n",
      "      41.7807551    37.94636735]\n",
      "   [  78.33663265   76.34353061   74.58165306 ...,   45.87120408\n",
      "      43.48989796   40.48187755]\n",
      "   ..., \n",
      "   [  80.46373469   78.43038776   79.09187755 ...,   10.71895918\n",
      "      37.28577551   42.45691837]\n",
      "   [  70.94057143   63.55583673   54.7444898  ...,   52.46567347\n",
      "      49.3945102    48.98463265]\n",
      "   [  70.26402041   62.41073469   51.14669388 ...,   68.1964898\n",
      "      69.43428571   68.35781633]]\n",
      "\n",
      "  [[  75.01826531   72.65195918   70.85671429 ...,   58.82467347\n",
      "      58.4197551    58.63465306]\n",
      "   [  74.78316327   72.63691837   70.91895918 ...,   56.96422449   56.544\n",
      "      55.70804082]\n",
      "   [  84.41510204   82.57302041   81.01312245 ...,   57.11087755\n",
      "      56.52940816   57.50165306]\n",
      "   ..., \n",
      "   [  73.06446939   71.28759184   73.21993878 ...,    8.95071429\n",
      "      34.22269388   38.1052449 ]\n",
      "   [  63.63295918   56.44293878   47.84238776 ...,   46.61495918\n",
      "      44.32565306   43.68942857]\n",
      "   [  62.04759184   55.35055102   44.24808163 ...,   58.26855102\n",
      "      63.37042857   64.13804082]]\n",
      "\n",
      "  [[ 107.52608163  104.24597959  102.52363265 ...,   91.70193878\n",
      "      90.1307551    88.21483673]\n",
      "   [ 107.72267347  105.63771429  104.00018367 ...,   89.21777551\n",
      "      87.61238776   85.64302041]\n",
      "   [ 113.81738776  111.05812245  109.59912245 ...,   87.79559184\n",
      "      87.02171429   85.84563265]\n",
      "   ..., \n",
      "   [  66.86355102   63.22930612   62.274      ...,   16.08228571\n",
      "      38.21557143   37.86181633]\n",
      "   [  55.35404082   46.32144898   34.84595918 ...,   47.70189796\n",
      "      43.25883673   40.37018367]\n",
      "   [  57.54336735   48.02361224   35.05283673 ...,   55.137        57.08042857\n",
      "      56.60042857]]]\n",
      "\n",
      "\n",
      " [[[  98.35810204  105.94430612  103.03944898 ...,   86.08416327\n",
      "      90.74489796   91.94502041]\n",
      "   [  91.97585714  109.73944898  102.86344898 ...,   93.07028571\n",
      "      97.7807551    80.94636735]\n",
      "   [  83.33663265  105.34353061  101.58165306 ...,   90.87120408\n",
      "      91.48989796   73.48187755]\n",
      "   ..., \n",
      "   [  23.46373469   15.43038776    8.09187755 ...,   99.71895918\n",
      "     105.28577551  115.45691837]\n",
      "   [   9.94057143    4.55583673   -0.2555102  ...,   55.46567347\n",
      "      76.3945102    85.98463265]\n",
      "   [  -5.73597959   -8.58926531   -6.85330612 ...,   52.1964898\n",
      "      37.43428571   36.35781633]]\n",
      "\n",
      "  [[  93.01826531  101.65195918   99.85671429 ...,   82.82467347\n",
      "      87.4197551    87.63465306]\n",
      "   [  85.78316327  104.63691837   98.91895918 ...,   87.96422449   93.544\n",
      "      76.70804082]\n",
      "   [  71.41510204   98.57302041   99.01312245 ...,   86.11087755\n",
      "      85.52940816   69.50165306]\n",
      "   ..., \n",
      "   [  17.06446939   11.28759184    4.21993878 ...,   98.95071429\n",
      "     104.22269388  116.1052449 ]\n",
      "   [   5.63295918    2.44293878   -3.15761224 ...,   55.61495918\n",
      "      76.32565306   85.68942857]\n",
      "   [  -7.95240816   -9.64944898   -9.75191837 ...,   51.26855102\n",
      "      38.37042857   37.13804082]]\n",
      "\n",
      "  [[ 106.52608163  115.24597959  114.52363265 ...,  100.70193878\n",
      "     102.1307551   101.21483673]\n",
      "   [  97.72267347  118.63771429  115.00018367 ...,  105.21777551\n",
      "     107.61238776   89.64302041]\n",
      "   [  80.81738776  110.05812245  114.59912245 ...,  102.79559184\n",
      "     103.02171429   85.84563265]\n",
      "   ..., \n",
      "   [  20.86355102   15.22930612    9.274      ...,  107.08228571\n",
      "     113.21557143  124.86181633]\n",
      "   [  11.35404082    7.32144898    2.84595918 ...,   65.70189796\n",
      "      85.25883673   93.37018367]\n",
      "   [  -1.45663265   -3.97638776   -2.94716327 ...,   59.137        48.08042857\n",
      "      46.60042857]]]]\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "# data = get_CIFAR10_data()\n",
    "# for k, v in data.iteritems():\n",
    "#   print '%s: ' % k, v.shape\n",
    "\n",
    "print data['X_val']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution: Naive forward pass\n",
    "The core of a convolutional network is the convolution operation. In the file `code/layers.py`, implement the forward pass for the convolution layer in the function `conv_forward_naive`. \n",
    "\n",
    "You don't have to worry too much about efficiency at this point; just write the code in whatever way you find most clear.\n",
    "\n",
    "You can test your implementation by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_forward_naive\n",
      "difference:  2.21214764175e-08\n"
     ]
    }
   ],
   "source": [
    "x_shape = (2, 3, 4, 4)\n",
    "w_shape = (3, 3, 4, 4)\n",
    "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n",
    "b = np.linspace(-0.1, 0.2, num=3)\n",
    "\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "out, _ = conv_forward_naive(x, w, b, conv_param)\n",
    "correct_out = np.array([[[[[-0.08759809, -0.10987781],\n",
    "                           [-0.18387192, -0.2109216 ]],\n",
    "                          [[ 0.21027089,  0.21661097],\n",
    "                           [ 0.22847626,  0.23004637]],\n",
    "                          [[ 0.50813986,  0.54309974],\n",
    "                           [ 0.64082444,  0.67101435]]],\n",
    "                         [[[-0.98053589, -1.03143541],\n",
    "                           [-1.19128892, -1.24695841]],\n",
    "                          [[ 0.69108355,  0.66880383],\n",
    "                           [ 0.59480972,  0.56776003]],\n",
    "                          [[ 2.36270298,  2.36904306],\n",
    "                           [ 2.38090835,  2.38247847]]]]])\n",
    "\n",
    "# Compare your output to ours; difference should be around 1e-8\n",
    "print 'Testing conv_forward_naive'\n",
    "print 'difference: ', rel_error(out, correct_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aside: Image processing via convolutions\n",
    "\n",
    "As fun way to both check your implementation and gain a better understanding of the type of operation that convolutional layers can perform, we will set up an input containing two images and manually set up filters that perform common image processing operations (grayscale conversion and edge detection). The convolution forward pass will apply these operations to each of the input images. We can then visualize the results as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(266, 266, 3)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'imshow_noax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d7c156418e96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Show the original images and the results of the conv operation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mimshow_noax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpuppy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Original image'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'imshow_noax' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM4AAADsCAYAAAA8e5n5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nf7FirqRaEAOVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107ae0c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.misc import imread, imresize\n",
    "\n",
    "kitten, puppy = imread('kitten.jpg'), imread('puppy.jpg')\n",
    "# kitten is wide, and puppy is already square\n",
    "d = kitten.shape[1] - kitten.shape[0]\n",
    "kitten_cropped = kitten[:, d/2:-d/2, :]\n",
    "\n",
    "# Remove center square pixels\n",
    "mask = np.ones((266, 266, 3))\n",
    "missing_start = int(266 * 0.25)\n",
    "missing_end = int(266 * 0.75)\n",
    "mask[missing_start:missing_end, missing_start:missing_end, :] = 0\n",
    "missing_kitten = kitten_cropped * mask\n",
    "\n",
    "mask = np.ones((517, 517, 3))\n",
    "missing_start = int(517 * 0.25)\n",
    "missing_end = int(517 * 0.75)\n",
    "mask[missing_start:missing_end, missing_start:missing_end, :] = 0\n",
    "missing_puppy = puppy * mask\n",
    "\n",
    "print kitten_cropped.shape\n",
    "    \n",
    "# Show the original images and the results of the conv operation\n",
    "plt.subplot(2, 3, 1)\n",
    "imshow_noax(puppy, normalize=False)\n",
    "plt.title('Original image')\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "imshow_noax(missing_puppy, normalize=False)\n",
    "plt.title('Missing pixels')\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "imshow_noax(kitten_cropped, normalize=False)\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "imshow_noax(missing_kitten, normalize=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution: Naive backward pass\n",
    "Implement the backward pass for the convolution operation in the function `conv_backward_naive` in the file `code/layers.py`. Again, you don't need to worry too much about computational efficiency.\n",
    "\n",
    "When you are done, run the following to check your backward pass with a numeric gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_backward_naive function\n",
      "dw error:  1.075219625e-10\n",
      "dx error:  3.63217597974e-09\n",
      "db error:  6.30578060178e-11\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(4, 3, 5, 5)\n",
    "w = np.random.randn(2, 3, 3, 3)\n",
    "b = np.random.randn(2,)\n",
    "dout = np.random.randn(4, 2, 5, 5)\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: conv_forward_naive(x, w, b, conv_param)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: conv_forward_naive(x, w, b, conv_param)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: conv_forward_naive(x, w, b, conv_param)[0], b, dout)\n",
    "\n",
    "out, cache = conv_forward_naive(x, w, b, conv_param)\n",
    "dx, dw, db = conv_backward_naive(dout, cache)\n",
    "\n",
    "# Your errors should be around 1e-9'\n",
    "print 'Testing conv_backward_naive function'\n",
    "print 'dw error: ', rel_error(dw, dw_num)\n",
    "print 'dx error: ', rel_error(dx, dx_num)\n",
    "print 'db error: ', rel_error(db, db_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max pooling: Naive forward\n",
    "Implement the forward pass for the max-pooling operation in the function `max_pool_forward_naive` in the file `code/layers.py`. Again, don't worry too much about computational efficiency.\n",
    "\n",
    "Check your implementation by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing max_pool_forward_naive function:\n",
      "difference:  4.16666651573e-08\n"
     ]
    }
   ],
   "source": [
    "x_shape = (2, 3, 4, 4)\n",
    "x = np.linspace(-0.3, 0.4, num=np.prod(x_shape)).reshape(x_shape)\n",
    "pool_param = {'pool_width': 2, 'pool_height': 2, 'stride': 2}\n",
    "\n",
    "out, _ = max_pool_forward_naive(x, pool_param)\n",
    "\n",
    "correct_out = np.array([[[[-0.26315789, -0.24842105],\n",
    "                          [-0.20421053, -0.18947368]],\n",
    "                         [[-0.14526316, -0.13052632],\n",
    "                          [-0.08631579, -0.07157895]],\n",
    "                         [[-0.02736842, -0.01263158],\n",
    "                          [ 0.03157895,  0.04631579]]],\n",
    "                        [[[ 0.09052632,  0.10526316],\n",
    "                          [ 0.14947368,  0.16421053]],\n",
    "                         [[ 0.20842105,  0.22315789],\n",
    "                          [ 0.26736842,  0.28210526]],\n",
    "                         [[ 0.32631579,  0.34105263],\n",
    "                          [ 0.38526316,  0.4       ]]]])\n",
    "\n",
    "# Compare your output with ours. Difference should be around 1e-8.\n",
    "print 'Testing max_pool_forward_naive function:'\n",
    "print 'difference: ', rel_error(out, correct_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max pooling: Naive backward\n",
    "Implement the backward pass for the max-pooling operation in the function `max_pool_backward_naive` in the file `code/layers.py`. You don't need to worry about computational efficiency.\n",
    "\n",
    "Check your implementation with numeric gradient checking by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing max_pool_backward_naive function:\n",
      "dx error:  3.27561780673e-12\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(3, 2, 8, 8)\n",
    "dout = np.random.randn(3, 2, 4, 4)\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: max_pool_forward_naive(x, pool_param)[0], x, dout)\n",
    "\n",
    "out, cache = max_pool_forward_naive(x, pool_param)\n",
    "dx = max_pool_backward_naive(dout, cache)\n",
    "\n",
    "# Your error should be around 1e-12\n",
    "print 'Testing max_pool_backward_naive function:'\n",
    "print 'dx error: ', rel_error(dx, dx_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast layers\n",
    "Making convolution and pooling layers fast can be challenging. To spare you the pain, we've provided fast implementations of the forward and backward passes for convolution and pooling layers in the file `code/fast_layers.py`.\n",
    "\n",
    "The fast convolution implementation depends on a Cython extension; to compile it you need to run the following from the `code` directory:\n",
    "\n",
    "```bash\n",
    "python setup.py build_ext --inplace\n",
    "```\n",
    "\n",
    "The API for the fast versions of the convolution and pooling layers is exactly the same as the naive versions that you implemented above: the forward pass receives data, weights, and parameters and produces outputs and a cache object; the backward pass recieves upstream derivatives and the cache object and produces gradients with respect to the data and weights.\n",
    "\n",
    "**NOTE:** The fast implementation for pooling will only perform optimally if the pooling regions are non-overlapping and tile the input. If these conditions are not met then the fast pooling implementation will not be much faster than the naive implementation.\n",
    "\n",
    "You can compare the performance of the naive and fast versions of these layers by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_forward_fast:\n",
      "Naive: 0.101198s\n",
      "Fast: 0.015907s\n",
      "Speedup: 6.361831x\n",
      "Difference:  2.16196659599e-11\n",
      "\n",
      "Testing conv_backward_fast:\n",
      "Naive: 0.124746s\n",
      "Fast: 0.009279s\n",
      "Speedup: 13.443896x\n",
      "dx difference:  5.46846343004e-12\n",
      "dw difference:  5.42820572768e-13\n",
      "db difference:  0.0\n"
     ]
    }
   ],
   "source": [
    "from code.fast_layers import conv_forward_fast, conv_backward_fast\n",
    "from time import time\n",
    "\n",
    "x = np.random.randn(100, 3, 31, 31)\n",
    "w = np.random.randn(25, 3, 3, 3)\n",
    "b = np.random.randn(25,)\n",
    "dout = np.random.randn(100, 25, 16, 16)\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "\n",
    "t0 = time()\n",
    "out_naive, cache_naive = conv_forward_naive(x, w, b, conv_param)\n",
    "t1 = time()\n",
    "out_fast, cache_fast = conv_forward_fast(x, w, b, conv_param)\n",
    "t2 = time()\n",
    "\n",
    "print 'Testing conv_forward_fast:'\n",
    "print 'Naive: %fs' % (t1 - t0)\n",
    "print 'Fast: %fs' % (t2 - t1)\n",
    "print 'Speedup: %fx' % ((t1 - t0) / (t2 - t1))\n",
    "print 'Difference: ', rel_error(out_naive, out_fast)\n",
    "\n",
    "t0 = time()\n",
    "dx_naive, dw_naive, db_naive = conv_backward_naive(dout, cache_naive)\n",
    "t1 = time()\n",
    "dx_fast, dw_fast, db_fast = conv_backward_fast(dout, cache_fast)\n",
    "t2 = time()\n",
    "\n",
    "print '\\nTesting conv_backward_fast:'\n",
    "print 'Naive: %fs' % (t1 - t0)\n",
    "print 'Fast: %fs' % (t2 - t1)\n",
    "print 'Speedup: %fx' % ((t1 - t0) / (t2 - t1))\n",
    "print 'dx difference: ', rel_error(dx_naive, dx_fast)\n",
    "print 'dw difference: ', rel_error(dw_naive, dw_fast)\n",
    "print 'db difference: ', rel_error(db_naive, db_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pool_forward_fast:\n",
      "Naive: 0.108164s\n",
      "fast: 0.003442s\n",
      "speedup: 31.424257x\n",
      "difference:  0.0\n",
      "\n",
      "Testing pool_backward_fast:\n",
      "Naive: 0.484727s\n",
      "speedup: 51.153403x\n",
      "dx difference:  0.0\n"
     ]
    }
   ],
   "source": [
    "from code.fast_layers import max_pool_forward_fast, max_pool_backward_fast\n",
    "\n",
    "x = np.random.randn(100, 3, 32, 32)\n",
    "dout = np.random.randn(100, 3, 16, 16)\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "t0 = time()\n",
    "out_naive, cache_naive = max_pool_forward_naive(x, pool_param)\n",
    "t1 = time()\n",
    "out_fast, cache_fast = max_pool_forward_fast(x, pool_param)\n",
    "t2 = time()\n",
    "\n",
    "print 'Testing pool_forward_fast:'\n",
    "print 'Naive: %fs' % (t1 - t0)\n",
    "print 'fast: %fs' % (t2 - t1)\n",
    "print 'speedup: %fx' % ((t1 - t0) / (t2 - t1))\n",
    "print 'difference: ', rel_error(out_naive, out_fast)\n",
    "\n",
    "t0 = time()\n",
    "dx_naive = max_pool_backward_naive(dout, cache_naive)\n",
    "t1 = time()\n",
    "dx_fast = max_pool_backward_fast(dout, cache_fast)\n",
    "t2 = time()\n",
    "\n",
    "print '\\nTesting pool_backward_fast:'\n",
    "print 'Naive: %fs' % (t1 - t0)\n",
    "print 'speedup: %fx' % ((t1 - t0) / (t2 - t1))\n",
    "print 'dx difference: ', rel_error(dx_naive, dx_fast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional \"sandwich\" layers\n",
    "Previously we introduced the concept of \"sandwich\" layers that combine multiple operations into commonly used patterns. In the file `code/layer_utils.py` you will find sandwich layers that implement a few commonly used patterns for convolutional networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_relu_pool\n",
      "dx error:  4.76189755828e-08\n",
      "dw error:  4.97229816946e-10\n",
      "db error:  2.60600470684e-11\n"
     ]
    }
   ],
   "source": [
    "from code.layer_utils import conv_relu_pool_forward, conv_relu_pool_backward\n",
    "\n",
    "x = np.random.randn(2, 3, 16, 16)\n",
    "w = np.random.randn(3, 3, 3, 3)\n",
    "b = np.random.randn(3,)\n",
    "dout = np.random.randn(2, 3, 8, 8)\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "out, cache = conv_relu_pool_forward(x, w, b, conv_param, pool_param)\n",
    "dx, dw, db = conv_relu_pool_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: conv_relu_pool_forward(x, w, b, conv_param, pool_param)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: conv_relu_pool_forward(x, w, b, conv_param, pool_param)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: conv_relu_pool_forward(x, w, b, conv_param, pool_param)[0], b, dout)\n",
    "\n",
    "print 'Testing conv_relu_pool'\n",
    "print 'dx error: ', rel_error(dx_num, dx)\n",
    "print 'dw error: ', rel_error(dw_num, dw)\n",
    "print 'db error: ', rel_error(db_num, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_relu:\n",
      "dx error:  3.07576107758e-09\n",
      "dw error:  5.64636131913e-10\n",
      "db error:  1.80131256247e-11\n"
     ]
    }
   ],
   "source": [
    "from code.layer_utils import conv_relu_forward, conv_relu_backward\n",
    "\n",
    "x = np.random.randn(2, 3, 8, 8)\n",
    "w = np.random.randn(3, 3, 3, 3)\n",
    "b = np.random.randn(3,)\n",
    "dout = np.random.randn(2, 3, 8, 8)\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "\n",
    "out, cache = conv_relu_forward(x, w, b, conv_param)\n",
    "dx, dw, db = conv_relu_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: conv_relu_forward(x, w, b, conv_param)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: conv_relu_forward(x, w, b, conv_param)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: conv_relu_forward(x, w, b, conv_param)[0], b, dout)\n",
    "\n",
    "print 'Testing conv_relu:'\n",
    "print 'dx error: ', rel_error(dx_num, dx)\n",
    "print 'dw error: ', rel_error(dw_num, dw)\n",
    "print 'db error: ', rel_error(db_num, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three-layer ConvNet\n",
    "Now that you have implemented all the necessary layers, we can put them together into a simple convolutional network.\n",
    "\n",
    "Open the file `code/cnn.py` and complete the implementation of the `ThreeLayerConvNet` class. Run the following cells to help you debug:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check loss\n",
    "After you build a new network, one of the first things you should do is sanity check the loss. When we use the softmax loss, we expect the loss for random weights (and no regularization) to be about `log(C)` for `C` classes. When we add regularization this should go up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss (no regularization):  2.30258402392\n",
      "Initial loss (with regularization):  2.50870892805\n"
     ]
    }
   ],
   "source": [
    "model = ThreeLayerConvNet()\n",
    "\n",
    "N = 50\n",
    "X = np.random.randn(N, 3, 32, 32)\n",
    "y = np.random.randint(10, size=N)\n",
    "\n",
    "loss, grads = model.loss(X, y)\n",
    "print 'Initial loss (no regularization): ', loss\n",
    "\n",
    "model.reg = 0.5\n",
    "loss, grads = model.loss(X, y)\n",
    "print 'Initial loss (with regularization): ', loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient check\n",
    "After the loss looks reasonable, use numeric gradient checking to make sure that your backward pass is correct. When you use numeric gradient checking you should use a small amount of artifical data and a small number of neurons at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 max relative error: 2.886487e-03\n",
      "W2 max relative error: 1.777449e-02\n",
      "W3 max relative error: 2.667549e-03\n",
      "b1 max relative error: 1.198817e-05\n",
      "b2 max relative error: 1.355508e-02\n",
      "b3 max relative error: 1.132811e-09\n"
     ]
    }
   ],
   "source": [
    "num_inputs = 2\n",
    "input_dim = (3, 16, 16)\n",
    "reg = 0.0\n",
    "num_classes = 10\n",
    "X = np.random.randn(num_inputs, *input_dim)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "model = ThreeLayerConvNet(num_filters=3, filter_size=3,\n",
    "                          input_dim=input_dim, hidden_dim=7,\n",
    "                          dtype=np.float64)\n",
    "loss, grads = model.loss(X, y)\n",
    "for param_name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, model.params[param_name], verbose=False, h=1e-6)\n",
    "    e = rel_error(param_grad_num, grads[param_name])\n",
    "    print '%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfit small data\n",
    "A nice trick is to train your model with just a few training samples. You should be able to overfit small datasets, which will result in very high training accuracy and comparatively low validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 20) loss: 2.252828\n",
      "(Epoch 0 / 10) train acc: 0.220000; val_acc: 0.137000\n",
      "(Iteration 2 / 20) loss: 4.859072\n",
      "(Epoch 1 / 10) train acc: 0.130000; val_acc: 0.077000\n",
      "(Iteration 3 / 20) loss: 4.062864\n",
      "(Iteration 4 / 20) loss: 2.976818\n",
      "(Epoch 2 / 10) train acc: 0.260000; val_acc: 0.113000\n",
      "(Iteration 5 / 20) loss: 2.546877\n",
      "(Iteration 6 / 20) loss: 3.104442\n",
      "(Epoch 3 / 10) train acc: 0.230000; val_acc: 0.124000\n",
      "(Iteration 7 / 20) loss: 2.372892\n",
      "(Iteration 8 / 20) loss: 2.304699\n",
      "(Epoch 4 / 10) train acc: 0.320000; val_acc: 0.168000\n",
      "(Iteration 9 / 20) loss: 2.003174\n",
      "(Iteration 10 / 20) loss: 2.022661\n",
      "(Epoch 5 / 10) train acc: 0.170000; val_acc: 0.089000\n",
      "(Iteration 11 / 20) loss: 2.235473\n",
      "(Iteration 12 / 20) loss: 1.918868\n",
      "(Epoch 6 / 10) train acc: 0.310000; val_acc: 0.165000\n",
      "(Iteration 13 / 20) loss: 1.837696\n",
      "(Iteration 14 / 20) loss: 1.783850\n",
      "(Epoch 7 / 10) train acc: 0.440000; val_acc: 0.206000\n",
      "(Iteration 15 / 20) loss: 1.588015\n",
      "(Iteration 16 / 20) loss: 1.593867\n",
      "(Epoch 8 / 10) train acc: 0.540000; val_acc: 0.200000\n",
      "(Iteration 17 / 20) loss: 1.667972\n",
      "(Iteration 18 / 20) loss: 1.371903\n",
      "(Epoch 9 / 10) train acc: 0.600000; val_acc: 0.175000\n",
      "(Iteration 19 / 20) loss: 1.442437\n",
      "(Iteration 20 / 20) loss: 1.491428\n",
      "(Epoch 10 / 10) train acc: 0.620000; val_acc: 0.171000\n"
     ]
    }
   ],
   "source": [
    "num_train = 100\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "model = ThreeLayerConvNet(weight_scale=1e-2)\n",
    "\n",
    "solver = Solver(model, small_data,\n",
    "                num_epochs=10, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=1)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the loss, training accuracy, and validation accuracy should show clear overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmUAAAHuCAYAAADeEHuhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2VCaSMQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106664b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(solver.train_acc_history, '-o')\n",
    "plt.plot(solver.val_acc_history, '-o')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the net\n",
    "By training the three-layer convolutional network for one epoch, you should achieve greater than 40% accuracy on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 980) loss: 2.304648\n",
      "(Epoch 0 / 1) train acc: 0.117000; val_acc: 0.115000\n",
      "(Iteration 21 / 980) loss: 2.071419\n",
      "(Iteration 41 / 980) loss: 1.922036\n",
      "(Iteration 61 / 980) loss: 2.100622\n",
      "(Iteration 81 / 980) loss: 1.768757\n",
      "(Iteration 101 / 980) loss: 1.799647\n",
      "(Iteration 121 / 980) loss: 2.000124\n",
      "(Iteration 141 / 980) loss: 2.113511\n",
      "(Iteration 161 / 980) loss: 1.646565\n",
      "(Iteration 181 / 980) loss: 1.789996\n",
      "(Iteration 201 / 980) loss: 1.836587\n",
      "(Iteration 221 / 980) loss: 1.590435\n",
      "(Iteration 241 / 980) loss: 1.831207\n",
      "(Iteration 261 / 980) loss: 1.853556\n",
      "(Iteration 281 / 980) loss: 1.638251\n",
      "(Iteration 301 / 980) loss: 1.556177\n",
      "(Iteration 321 / 980) loss: 1.880166\n",
      "(Iteration 341 / 980) loss: 1.618852\n",
      "(Iteration 361 / 980) loss: 1.657027\n",
      "(Iteration 381 / 980) loss: 1.668784\n",
      "(Iteration 401 / 980) loss: 1.553513\n",
      "(Iteration 421 / 980) loss: 1.508858\n",
      "(Iteration 441 / 980) loss: 1.557400\n",
      "(Iteration 461 / 980) loss: 1.590944\n",
      "(Iteration 481 / 980) loss: 1.806477\n",
      "(Iteration 501 / 980) loss: 1.742890\n",
      "(Iteration 521 / 980) loss: 1.415435\n",
      "(Iteration 541 / 980) loss: 1.433470\n",
      "(Iteration 561 / 980) loss: 1.573982\n",
      "(Iteration 581 / 980) loss: 1.777100\n",
      "(Iteration 601 / 980) loss: 1.610263\n",
      "(Iteration 621 / 980) loss: 1.932574\n",
      "(Iteration 641 / 980) loss: 1.512228\n",
      "(Iteration 661 / 980) loss: 1.449554\n",
      "(Iteration 681 / 980) loss: 1.574673\n",
      "(Iteration 701 / 980) loss: 1.362345\n",
      "(Iteration 721 / 980) loss: 1.501212\n",
      "(Iteration 741 / 980) loss: 1.733210\n",
      "(Iteration 761 / 980) loss: 1.407071\n",
      "(Iteration 781 / 980) loss: 1.497118\n",
      "(Iteration 801 / 980) loss: 1.539015\n",
      "(Iteration 821 / 980) loss: 1.740901\n",
      "(Iteration 841 / 980) loss: 1.517913\n",
      "(Iteration 861 / 980) loss: 1.837417\n",
      "(Iteration 881 / 980) loss: 1.626538\n",
      "(Iteration 901 / 980) loss: 1.570535\n",
      "(Iteration 921 / 980) loss: 1.561479\n",
      "(Iteration 941 / 980) loss: 1.389299\n",
      "(Iteration 961 / 980) loss: 1.599823\n",
      "(Epoch 1 / 1) train acc: 0.478000; val_acc: 0.484000\n"
     ]
    }
   ],
   "source": [
    "model = ThreeLayerConvNet(weight_scale=0.001, hidden_dim=500, reg=0.001)\n",
    "\n",
    "solver = Solver(model, data,\n",
    "                num_epochs=1, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=20)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Filters\n",
    "You can visualize the first-layer convolutional filters from the trained network by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATYAAAE1CAYAAABtKMwHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112796410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from code.vis_utils import visualize_grid\n",
    "\n",
    "grid = visualize_grid(model.params['W1'].transpose(0, 2, 3, 1))\n",
    "plt.imshow(grid.astype('uint8'))\n",
    "plt.axis('off')\n",
    "plt.gcf().set_size_inches(5, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Batch Normalization\n",
    "We already saw that batch normalization is a very useful technique for training deep fully-connected networks. Batch normalization can also be used for convolutional networks, but we need to tweak it a bit; the modification will be called \"spatial batch normalization.\"\n",
    "\n",
    "Normally batch-normalization accepts inputs of shape `(N, D)` and produces outputs of shape `(N, D)`, where we normalize across the minibatch dimension `N`. For data coming from convolutional layers, batch normalization needs to accept inputs of shape `(N, C, H, W)` and produce outputs of shape `(N, C, H, W)` where the `N` dimension gives the minibatch size and the `(H, W)` dimensions give the spatial size of the feature map.\n",
    "\n",
    "If the feature map was produced using convolutions, then we expect the statistics of each feature channel to be relatively consistent both between different imagesand different locations within the same image. Therefore spatial batch normalization computes a mean and variance for each of the `C` feature channels by computing statistics over both the minibatch dimension `N` and the spatial dimensions `H` and `W`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial batch normalization: forward\n",
    "\n",
    "In the file `code/layers.py`, implement the forward pass for spatial batch normalization in the function `spatial_batchnorm_forward`. Check your implementation by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before spatial batch normalization:\n",
      "  Shape:  (2, 3, 4, 5)\n",
      "  Means:  [ 10.48023128   9.8985406    9.9931654 ]\n",
      "  Stds:  [ 3.72615063  3.78527943  4.45620815]\n",
      "After spatial batch normalization:\n",
      "  Shape:  (2, 3, 4, 5)\n",
      "  Means:  [  1.33226763e-16  -4.08006962e-16   2.16493490e-16]\n",
      "  Stds:  [ 0.99999964  0.99999965  0.99999975]\n",
      "After spatial batch normalization (nontrivial gamma, beta):\n",
      "  Shape:  (2, 3, 4, 5)\n",
      "  Means:  [ 6.  7.  8.]\n",
      "  Stds:  [ 2.99999892  3.9999986   4.99999874]\n"
     ]
    }
   ],
   "source": [
    "# Check the training-time forward pass by checking means and variances\n",
    "# of features both before and after spatial batch normalization\n",
    "\n",
    "N, C, H, W = 2, 3, 4, 5\n",
    "x = 4 * np.random.randn(N, C, H, W) + 10\n",
    "\n",
    "print 'Before spatial batch normalization:'\n",
    "print '  Shape: ', x.shape\n",
    "print '  Means: ', x.mean(axis=(0, 2, 3))\n",
    "print '  Stds: ', x.std(axis=(0, 2, 3))\n",
    "\n",
    "# Means should be close to zero and stds close to one\n",
    "gamma, beta = np.ones(C), np.zeros(C)\n",
    "bn_param = {'mode': 'train'}\n",
    "out, _ = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
    "print 'After spatial batch normalization:'\n",
    "print '  Shape: ', out.shape\n",
    "print '  Means: ', out.mean(axis=(0, 2, 3))\n",
    "print '  Stds: ', out.std(axis=(0, 2, 3))\n",
    "\n",
    "# Means should be close to beta and stds close to gamma\n",
    "gamma, beta = np.asarray([3, 4, 5]), np.asarray([6, 7, 8])\n",
    "out, _ = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
    "print 'After spatial batch normalization (nontrivial gamma, beta):'\n",
    "print '  Shape: ', out.shape\n",
    "print '  Means: ', out.mean(axis=(0, 2, 3))\n",
    "print '  Stds: ', out.std(axis=(0, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After spatial batch normalization (test-time):\n",
      "  means:  [ 0.0426927   0.0352823   0.04094736  0.05720595]\n",
      "  stds:  [ 0.99220873  0.98252597  0.98189351  0.97121617]\n"
     ]
    }
   ],
   "source": [
    "# Check the test-time forward pass by running the training-time\n",
    "# forward pass many times to warm up the running averages, and then\n",
    "# checking the means and variances of activations after a test-time\n",
    "# forward pass.\n",
    "\n",
    "N, C, H, W = 10, 4, 11, 12\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "gamma = np.ones(C)\n",
    "beta = np.zeros(C)\n",
    "for t in xrange(50):\n",
    "  x = 2.3 * np.random.randn(N, C, H, W) + 13\n",
    "  spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
    "bn_param['mode'] = 'test'\n",
    "x = 2.3 * np.random.randn(N, C, H, W) + 13\n",
    "a_norm, _ = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
    "\n",
    "# Means should be close to zero and stds close to one, but will be\n",
    "# noisier than training-time forward passes.\n",
    "print 'After spatial batch normalization (test-time):'\n",
    "print '  means: ', a_norm.mean(axis=(0, 2, 3))\n",
    "print '  stds: ', a_norm.std(axis=(0, 2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial batch normalization: backward\n",
    "In the file `code/layers.py`, implement the backward pass for spatial batch normalization in the function `spatial_batchnorm_backward`. Run the following to check your implementation using a numeric gradient check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  2.03831912508e-08\n",
      "dgamma error:  9.17329273285e-12\n",
      "dbeta error:  3.2826568995e-12\n"
     ]
    }
   ],
   "source": [
    "N, C, H, W = 2, 3, 4, 5\n",
    "x = 5 * np.random.randn(N, C, H, W) + 12\n",
    "gamma = np.random.randn(C)\n",
    "beta = np.random.randn(C)\n",
    "dout = np.random.randn(N, C, H, W)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "fx = lambda x: spatial_batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fg = lambda a: spatial_batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fb = lambda b: spatial_batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "da_num = eval_numerical_gradient_array(fg, gamma, dout)\n",
    "db_num = eval_numerical_gradient_array(fb, beta, dout)\n",
    "\n",
    "_, cache = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
    "dx, dgamma, dbeta = spatial_batchnorm_backward(dout, cache)\n",
    "print 'dx error: ', rel_error(dx_num, dx)\n",
    "print 'dgamma error: ', rel_error(da_num, dgamma)\n",
    "print 'dbeta error: ', rel_error(db_num, dbeta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment!\n",
    "Experiment and try to get the best performance that you can on CIFAR-10 using a ConvNet. Here are some ideas to get you started:\n",
    "\n",
    "### Things you should try:\n",
    "- Filter size: Above we used 7x7; this makes pretty pictures but smaller filters may be more efficient\n",
    "- Number of filters: Above we used 32 filters. Do more or fewer do better?\n",
    "- Batch normalization: Try adding spatial batch normalization after convolution layers and vanilla batch normalization aafter affine layers. Do your networks train faster?\n",
    "- Network architecture: The network above has two layers of trainable parameters. Can you do better with a deeper network? You can implement alternative architectures in the file `code/classifiers/convnet.py`. Some good architectures to try include:\n",
    "    - [conv-relu-pool]xN - conv - relu - [affine]xM - [softmax or SVM]\n",
    "    - [conv-relu-pool]XN - [affine]XM - [softmax or SVM]\n",
    "    - [conv-relu-conv-relu-pool]xN - [affine]xM - [softmax or SVM]\n",
    "\n",
    "### Tips for training\n",
    "For each network architecture that you try, you should tune the learning rate and regularization strength. When doing this there are a couple important things to keep in mind:\n",
    "\n",
    "- If the parameters are working well, you should see improvement within a few hundred iterations\n",
    "- Remember the course-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n",
    "- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
    "\n",
    "### Going above and beyond\n",
    "If you are feeling adventurous there are many other features you can implement to try and improve your performance. You are **not required** to implement any of these; however they would be good things to try for extra credit.\n",
    "\n",
    "- Alternative update steps: For the assignment we implemented SGD+momentum, RMSprop, and Adam; you could try alternatives like AdaGrad or AdaDelta.\n",
    "- Alternative activation functions such as leaky ReLU, parametric ReLU, or MaxOut.\n",
    "- Model ensembles\n",
    "- Data augmentation\n",
    "\n",
    "If you do decide to implement something extra, clearly describe it in the \"Extra Credit Description\" cell below.\n",
    "\n",
    "### What we expect\n",
    "At the very least, you should be able to train a ConvNet that gets at least 65% accuracy on the validation set. This is just a lower bound - if you are careful it should be possible to get accuracies much higher than that! Extra credit points will be awarded for particularly high-scoring models or unique approaches.\n",
    "\n",
    "You should use the space below to experiment and train your network. The final cell in this notebook should contain the training, validation, and test set accuracies for your final trained network. In this notebook you should also write an explanation of what you did, any additional features that you implemented, and any visualizations or graphs that you make in the process of training and evaluating your network.\n",
    "\n",
    "Have fun and happy training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 980) loss: 2.306510\n",
      "(Epoch 0 / 1) train acc: 0.120000; val_acc: 0.078000\n",
      "(Iteration 21 / 980) loss: 2.002744\n",
      "(Iteration 41 / 980) loss: 2.112449\n",
      "(Iteration 61 / 980) loss: 1.876420\n",
      "(Iteration 81 / 980) loss: 1.912982\n",
      "(Iteration 101 / 980) loss: 1.559470\n",
      "(Iteration 121 / 980) loss: 1.766999\n",
      "(Iteration 141 / 980) loss: 1.667226\n",
      "(Iteration 161 / 980) loss: 1.604262\n",
      "(Iteration 181 / 980) loss: 1.829723\n",
      "(Iteration 201 / 980) loss: 1.326141\n",
      "(Iteration 221 / 980) loss: 1.669578\n",
      "(Iteration 241 / 980) loss: 1.697812\n",
      "(Iteration 261 / 980) loss: 1.502303\n",
      "(Iteration 281 / 980) loss: 1.468298\n",
      "(Iteration 301 / 980) loss: 1.735126\n",
      "(Iteration 321 / 980) loss: 1.705144\n",
      "(Iteration 341 / 980) loss: 1.428151\n",
      "(Iteration 361 / 980) loss: 1.629662\n",
      "(Iteration 381 / 980) loss: 1.120795\n",
      "(Iteration 401 / 980) loss: 1.652144\n",
      "(Iteration 421 / 980) loss: 1.491282\n",
      "(Iteration 441 / 980) loss: 1.654344\n",
      "(Iteration 461 / 980) loss: 1.321427\n",
      "(Iteration 481 / 980) loss: 1.558859\n",
      "(Iteration 501 / 980) loss: 1.752879\n",
      "(Iteration 521 / 980) loss: 1.467809\n",
      "(Iteration 541 / 980) loss: 1.567111\n",
      "(Iteration 561 / 980) loss: 1.594592\n",
      "(Iteration 581 / 980) loss: 1.365413\n",
      "(Iteration 601 / 980) loss: 1.630039\n",
      "(Iteration 621 / 980) loss: 1.847397\n",
      "(Iteration 641 / 980) loss: 1.489859\n",
      "(Iteration 661 / 980) loss: 1.604622\n",
      "(Iteration 681 / 980) loss: 1.288680\n",
      "(Iteration 701 / 980) loss: 1.781055\n",
      "(Iteration 721 / 980) loss: 1.571125\n",
      "(Iteration 741 / 980) loss: 1.477244\n",
      "(Iteration 761 / 980) loss: 1.770783\n",
      "(Iteration 781 / 980) loss: 1.509995\n",
      "(Iteration 801 / 980) loss: 1.562207\n",
      "(Iteration 821 / 980) loss: 1.778559\n",
      "(Iteration 841 / 980) loss: 1.311077\n",
      "(Iteration 861 / 980) loss: 1.386148\n",
      "(Iteration 881 / 980) loss: 1.363861\n",
      "(Iteration 901 / 980) loss: 1.326632\n",
      "(Iteration 921 / 980) loss: 1.546754\n",
      "(Iteration 941 / 980) loss: 1.498746\n",
      "(Iteration 961 / 980) loss: 1.483621\n",
      "(Epoch 1 / 1) train acc: 0.498000; val_acc: 0.504000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.48499999999999999"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a really good model on CIFAR-10\n",
    "model = ThreeLayerConvNet(weight_scale=0.001, hidden_dim=500, reg=0.001, filter_size=3, num_filters=60)\n",
    "\n",
    "solver = Solver(model, data,\n",
    "                num_epochs=1, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=20)\n",
    "solver.train()\n",
    "solver.check_accuracy(data['X_test'], data['y_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit Description\n",
    "If you implement any additional features for extra credit, clearly describe them here with pointers to any code in this or other files if applicable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
